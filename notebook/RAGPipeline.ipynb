{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6385c7",
   "metadata": {},
   "source": [
    "##### <code><b><i>RAG Pipeline - Data Ingetion to VectorDB Pipeline </i></b></code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c32cf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohsi\\Desktop\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_community.document_loaders import PyMuPDFLoader,PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd06f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document: Aptitude Book.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 105 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 139 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 195 0 (offset 0)\n",
      "Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Ignoring wrong pointing object 560 0 (offset 0)\n",
      "Ignoring wrong pointing object 767 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loaded 212 pages from Aptitude Book.pdf\n",
      "Loading document: Resume_DataScience.pdf\n",
      "-> Loaded 1 pages from Resume_DataScience.pdf\n",
      "Total documents loaded: 213\n"
     ]
    }
   ],
   "source": [
    "# Load all PDF files from the data directory\n",
    "def load_pdfs_from_directory(directory_path):\n",
    "  pdf_files = Path(directory_path).glob(\"*.pdf\")\n",
    "  all_documents = []\n",
    "  \n",
    "  for pdf_file in pdf_files:\n",
    "    print(f\"Loading document: {pdf_file.name}\")\n",
    "    try:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "          doc.metadata[\"source\"] = pdf_file.name\n",
    "          doc.metadata[\"file_type\"] = 'pdf'\n",
    "          \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"-> Loaded {len(documents)} pages from {pdf_file.name}\")\n",
    "    except Exception:\n",
    "        print(f\"Failed to load {pdf_file.name}\")\n",
    "  \n",
    "  print(f\"Total documents loaded: {len(all_documents)}\")\n",
    "  return all_documents\n",
    "\n",
    "all_documents = load_pdfs_from_directory(\"../data/pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709fe1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents after splitting: 832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20230415170124Z00'00'\", 'title': 'Microsoft Word - Workbook.docx', 'moddate': \"D:20230415170124Z00'00'\", 'source': 'Aptitude Book.pdf', 'total_pages': 212, 'page': 0, 'page_label': '1', 'file_type': 'pdf'}, page_content='1 \\n             THE APTITUDE TRIAD Mastering Quantitative, Logical, and Verbal Skills'),\n",
       " Document(metadata={'producer': 'macOS Version 13.2.1 (Build 22D68) Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20230415170124Z00'00'\", 'title': 'Microsoft Word - Workbook.docx', 'moddate': \"D:20230415170124Z00'00'\", 'source': 'Aptitude Book.pdf', 'total_pages': 212, 'page': 1, 'page_label': '2', 'file_type': 'pdf'}, page_content='2 \\nTABLE OF CONTENTS  SECTION A – QUANTITATIVE APTITUDE Module Topic    Page No. Module 1 Number System 5 Module 2 HCF, LCM and Decimal Fractions 11 Module 3 Simplification 15 Module 4 Percentages 18 Module 5 Profit, Loss and Discounts 23 Module 6 Simple and Compound Interest 28 Module 7 Averages 33 Module 8 Alligations and Mixtures 37 Module 9 Ratios, Proportions and Variations 41 Module 10 Partnership 45 Module 11 Time and Work 48 Module 12 Time, Speed and Distance 54 Module 13 Trains, Boats and Streams, Races 59 Module 14 Permutation and Combination 67 Module 15 Probability 75 Module 16 Data Interpretation 82 Module 17 Ages 87   ‘')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Splitting\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=chunk_size,\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      length_function=len,\n",
    "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "  )\n",
    "  \n",
    "  split_docs = text_splitter.split_documents(documents)\n",
    "  print(f\"Total documents after splitting: {len(split_docs)}\")  \n",
    "  return split_docs\n",
    "\n",
    "chunks = split_documents(all_documents)\n",
    "chunks[:2]  # Display first two split documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b910a2",
   "metadata": {},
   "source": [
    "##### <code><b><i>Embedding and VectorDB</i></b></code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93764bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uuid\n",
    "import chromadb \n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4facffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully. 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x228c005ede0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "  def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "      self.model_name = model_name\n",
    "      self.model = None\n",
    "      self._load_model()\n",
    "      \n",
    "  def _load_model(self):\n",
    "    try:\n",
    "      print(f\"Loading embedding model: {self.model_name}...\")\n",
    "      self.model = SentenceTransformer(self.model_name)\n",
    "      print(\"Model loaded successfully.\",self.model.get_sentence_embedding_dimension())\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading model: {self.model_name}. Exception: {e}\")\n",
    "      raise\n",
    "    \n",
    "  def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Embedding model is not loaded.\")\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    embeddings = self.model.encode(texts, convert_to_numpy=True) \n",
    "    print(\"Gentrated embeddings for shape:\",embeddings.shape)\n",
    "    return embeddings\n",
    "  \n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bed7f",
   "metadata": {},
   "source": [
    "##### <code><b><i>VectorStore</i></b></code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c690168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized at ../data/vector_db with collection 'pdf_documents'.\n",
      "Current number of vectors in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x228bff280e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "  def __init__(self, collection_name: str = \"pdf_documents\",presistence_dir: str = \"../data/vector_db\"):\n",
    "    self.collection_name = collection_name\n",
    "    self.persistance_dir = presistence_dir\n",
    "    self.client = None\n",
    "    self.collection = None\n",
    "    self._initialize_vector_store()\n",
    "    \n",
    "  def _initialize_vector_store(self):\n",
    "    try:\n",
    "      os.makedirs(self.persistance_dir, exist_ok=True)\n",
    "      self.client = chromadb.PersistentClient(path=self.persistance_dir)\n",
    "      \n",
    "      self.collection = self.client.get_or_create_collection(\n",
    "        name=self.collection_name,\n",
    "        metadata={\"discription\": \"Collection of PDF document embeddings\"}\n",
    "        )\n",
    "      \n",
    "      print(f\"Vector store initialized at {self.persistance_dir} with collection '{self.collection_name}'.\")\n",
    "      print(f\"Current number of vectors in collection: {self.collection.count()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error initializing vector store: {e}\")\n",
    "      raise    \n",
    "    \n",
    "  def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "    if len(documents) != len(embeddings):\n",
    "      raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "    print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "    \n",
    "    #Prepare data for ChromaDB\n",
    "    ids = []\n",
    "    metadatas = []\n",
    "    document_text = []\n",
    "    embedding_list = []\n",
    "    \n",
    "    for i,(doc,embadding) in enumerate(zip(documents,embeddings)):\n",
    "      # Generate a unique ID for each document\n",
    "      doc_id = f'doc_{ uuid.uuid4().hex[:8]}_{i}'\n",
    "      ids.append(doc_id)\n",
    "      \n",
    "      #Prepare metadata\n",
    "      metadata = dict(doc.metadata)\n",
    "      metadata[\"doc_index\"] = i\n",
    "      metadata[\"conttent_length\"] = len(doc.page_content)\n",
    "      metadatas.append(metadata)\n",
    "      \n",
    "      #Document content\n",
    "      document_text.append(doc.page_content)\n",
    "      \n",
    "      # Embedding\n",
    "      embedding_list.append(embadding.tolist())\n",
    "      \n",
    "    # Add to Collection\n",
    "    try:\n",
    "      self.collection.add(\n",
    "        ids=ids,\n",
    "        metadatas=metadatas,\n",
    "        documents=document_text,\n",
    "        embeddings=embedding_list\n",
    "      )\n",
    "      print(f\"Successfully added {len(documents)} documents to vector store.\")\n",
    "      print(f\"Total documents in collection now: {self.collection.count()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error adding documents to vector store: {e}\")\n",
    "      raise\n",
    "    \n",
    "vectorStore = VectorStore()\n",
    "vectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43cbb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 832 texts...\n",
      "Gentrated embeddings for shape: (832, 384)\n",
      "Adding 832 documents to vector store...\n",
      "Successfully added 832 documents to vector store.\n",
      "Total documents in collection now: 832\n"
     ]
    }
   ],
   "source": [
    "#Convert the text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "#Generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "#Store in vector DB\n",
    "vectorStore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c4f59",
   "metadata": {},
   "source": [
    "##### <code><b><i>Retriever Pipeline from Vectorstore</i></b></code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b227831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "  def __init__(self,vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "    self.vector_store = vector_store\n",
    "    self.embedding_manager = embedding_manager\n",
    "    \n",
    "  def retrieve(self, query: str, top_k: int = 5,score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "    print(f\"Retrieving top {top_k} documents for query: '{query}' and score threshold: {score_threshold}\")\n",
    "    #Generate embedding for the query\n",
    "    query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "    \n",
    "    #Search in vector store\n",
    "    try:\n",
    "      results = self.vector_store.collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "      )\n",
    "      \n",
    "      retrieved_docs = []\n",
    "      \n",
    "      if results[\"documents\"] and results[\"documents\"][0]:\n",
    "        documents = results[\"documents\"][0]\n",
    "        metadatas = results[\"metadatas\"][0]\n",
    "        distances = results[\"distances\"][0]\n",
    "        ids = results[\"ids\"][0]\n",
    "        \n",
    "        for i,(doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "          \n",
    "          score = 1 - distance  # Convert distance to similarity score\n",
    "          if score >= score_threshold:\n",
    "            retrieved_docs.append({\n",
    "              \"id\": doc_id,\n",
    "              \"content\": document ,\n",
    "              \"metadata\": metadata,\n",
    "              \"similarity_score\": score,\n",
    "              \"rank\": i + 1\n",
    "            })\n",
    "        print(f\"Retrieved {len(retrieved_docs)} documents after applying score threshold.\")\n",
    "      else:\n",
    "        print(\"No documents retrieved from vector store.\")\n",
    "      return retrieved_docs \n",
    "    \n",
    "    except Exception as e:\n",
    "      print(f\"Error during retrieval: {e}\")\n",
    "      return []\n",
    "  \n",
    "rag_retriever = RAGRetriever(vectorStore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fbbcc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving top 5 documents for query: 'What is odd number?' and score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n",
      "Gentrated embeddings for shape: (1, 384)\n",
      "Retrieved 4 documents after applying score threshold.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = rag_retriever.retrieve(\"What is odd number?\", top_k=5, score_threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1ee5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
